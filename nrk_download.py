#!/usr/bin/env pythonimport reimport osimport ioimport sysimport requestsimport datetimeimport getoptfrom bs4 import BeautifulSoupfrom libs import hlsdef usage():    print('nrk_download.py -u <url>]')def progress(p):    sys.stdout.write("\rProgress: %d%%" % p)    sys.stdout.flush()    def xml2srt(text=''):    soup = BeautifulSoup(text)    result = u''    zerotime = datetime.datetime.strptime("0", "%H")    # For simple converting to timedelta    entries_skipped = 0                                 # To maintain continuous index increment even when fail    for i, p in enumerate(soup('p'), start=1):        try:                                            # Sometimes malformed with negative duration            begin = datetime.datetime.strptime(p['begin'], '%H:%M:%S.%f')            end = begin + (datetime.datetime.strptime(p['dur'], '%H:%M:%S.%f') - zerotime)        except:            entries_skipped += 1            continue        if(begin.hour >= 10):                           # Assume erroneous 10 hour tape offset            begin -= datetime.timedelta(hours = 10)        section = u'' + str(i - entries_skipped) + '\n'    # u'' to make sure it's unicode in both python 2.7 and 3        section += '%s,%03d' % (begin.strftime('%H:%M:%S'), begin.microsecond/1000)        section += ' --> '        section += '%s,%03d' % (end.strftime('%H:%M:%S'), end.microsecond/1000)        section += '\n'        pcont = p.decode()        pcont = re.sub('<p [^>]*>', '', pcont)        pcont = re.sub('</p>', '', pcont)        pcont = re.sub('<br ?/>', '\n', pcont)        pcont = pcont.replace('<span style="italic"> ', '<i>').replace(' </span>', '</i>')        section += pcont        section = section.strip()        section = re.sub('[\n]{2,}', '\n', section)        result += section + '\n\n'                return resultdef get_req(url):    try:        req = requests.get(url)    except requests.exceptions.MissingSchema:        req = get_req("http://" + url)    except requests.exceptions.RequestException as e:        sys.exit(e)        return reqdef main(argv):    print("\nNRK Download\n")    u_arg_passed = False    subs_exist = False    try:        opts, args = getopt.getopt(argv,"hu:")    except getopt.GetoptError:        usage()        sys.exit(2)    for opt, arg in opts:        if opt == '-h':            usage()            sys.exit(0)        elif opt == '-u':            url = arg            u_arg_passed = True        if not u_arg_passed:        usage()        sys.exit(2)        req = get_req(url)    html_doc = req.text        soup = BeautifulSoup(html_doc)    title_meta = soup.find("meta", attrs={"name" : "title"})    episode_number_meta = soup.find("meta", attrs={"name" : "episodenumber"})    if not title_meta:        sys.exit("Error: Did not recognize HTML structure. Check your url.")        title = title_meta["content"].strip()    out_filename = re.sub('[/\\\?%\*:|"<>]', '_', title)   # not allowed: / \ ? % * : | " < >    if episode_number_meta:        out_filename += " " + episode_number_meta["content"].strip()            p_div = soup.find(id="playerelement")    if not p_div:        sys.exit("Error: Did not recognize HTML structure. Check your url.")    print("Found: " + title + '\n')    videolink = p_div.get('data-hls-media').split('?')[0]    if p_div.get('data-subtitlesurl'):        print("Saving " + out_filename + ".srt")        subs_exist = True        suburl = 'http://' + req.url.split('/')[2] + p_div.get('data-subtitlesurl')        sub_xml = get_req(suburl).text        sub_srt = xml2srt(sub_xml)            srtfile = io.open(out_filename + '.srt', 'w')        srtfile.write(sub_srt)        srtfile.close()    print("Saving " + out_filename + ".ts\n")        hls.dump(videolink, out_filename + ".ts", progress)        print('\n')if __name__ == "__main__":    main(sys.argv[1:])