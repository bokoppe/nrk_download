#!/usr/bin/env pythonimport reimport osimport ioimport sysimport urllib2import datetimeimport getoptfrom bs4 import BeautifulSoupfrom libs import hlsdef usage():    print 'nrk_download.py -u <url>]'def progress(p):    sys.stdout.write("\rProgress: %d%%" % p)    sys.stdout.flush()    def xml2srt(text=''):    soup = BeautifulSoup(text)    result = u''    zerotime = datetime.datetime.strptime("0", "%H")    # For simple converting to timedelta    entries_skipped = 0                                 # To maintain continuous index increment even when fail    for i, p in enumerate(soup('p'), start=1):        try:                                            # Sometimes malformed with negative duration            begin = datetime.datetime.strptime(p['begin'], '%H:%M:%S.%f')            end = begin + (datetime.datetime.strptime(p['dur'], '%H:%M:%S.%f') - zerotime)        except:            entries_skipped += 1            continue        section = unicode(i - entries_skipped) + '\n'        section += '%s,%03d' % (begin.strftime('%H:%M:%S'), begin.microsecond/1000)        section += ' --> '        section += '%s,%03d' % (end.strftime('%H:%M:%S'), end.microsecond/1000)        section += '\n'        pcont = unicode(p)        pcont = re.sub('<p [^>]*>', '', pcont)        pcont = re.sub('</p>', '', pcont)        pcont = re.sub('<br ?/>', '\n', pcont)        pcont = pcont.replace('<span style="italic"> ', '<i>').replace(' </span>', '</i>')        section += pcont        section = section.strip()        section = re.sub('[\n]{2,}', '\n', section)        result += section + '\n\n'                return resultdef main(argv):    u_arg_passed = False    subs_exist = False    try:        opts, args = getopt.getopt(argv,"hu:")    except getopt.GetoptError:        usage()        sys.exit(2)    for opt, arg in opts:        if opt == '-h':            usage()            sys.exit(0)        elif opt == '-u':            url = arg            u_arg_passed = True        if not u_arg_passed:        usage()        sys.exit(2)        try:        html_doc = urllib2.urlopen(url).read()    except urllib2.HTTPError as e:        sys.exit("Error: Server could not fulfill request.\nHTTP Error code: " + e.code)    except urllib2.URLError as e:        sys.exit("Error: Could not reach server.\nReason: " + e.reason)            soup = BeautifulSoup(html_doc)    title_meta = soup.find("meta", attrs={"name" : "title"})    episode_number_meta = soup.find("meta", attrs={"name" : "episodenumber"})    if not title_meta:        sys.exit("Error: Did not recognize HTML structure. Check your url.")        title = title_meta["content"].strip()    out_filename = re.sub('[/\\\?%\*:|"<>]', '_', title)   # ikke lov: / \ ? % * : | " < >    if episode_number_meta:        out_filename += " " + episode_number_meta["content"].strip()            p_div = soup.find(id="playerelement")    if not p_div:        sys.exit("Error: Did not recognize HTML structure. Check your url.")    videolink = p_div.get('data-media').replace('manifest.f4m', 'master.m3u8')    videolink = re.sub('/\w/', '/i/', videolink)    if p_div.get('data-subtitlesurl'):        subs_exist = True        suburl = 'http://' + urllib2.urlparse.urlparse(url).netloc + p_div.get('data-subtitlesurl')        sub_xml = urllib2.urlopen(suburl).read()        sub_srt = xml2srt(sub_xml)            srtfile = io.open(out_filename + '.srt', 'w')#, encoding="utf-8")        srtfile.write(sub_srt)        srtfile.close()    print "NRK Download"    print "Found:", title    print "Saving as:", out_filename + ".ts\n"        hls.dump(videolink, out_filename + ".ts", progress)        print '\n'if __name__ == "__main__":    main(sys.argv[1:])